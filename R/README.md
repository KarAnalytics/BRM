Incomplete data with blockwise missing patterns are commonly encountered in analytics, and solutions typically entail listwise deletion or    imputation. However, as the proportion of missing values in input features increases, listwise or columnwise deletion leads to information loss, whereas    imputation diminishes the integrity of the training data set. We present the blockwise reduced modeling (BRM) method for analyzing blockwise missing patterns, which adapts and improves on the notion of reduced modeling proposed by Friedman, Kohavi, and Yun in 1996 as lazy decision trees. In contrast to the original idea of reduced modeling of delaying model induction until a prediction is required, our method is significantly faster because it exploits the blockwise missing patterns to pretrain ensemble models that require minimum imputation of data. Models are pretrained over the overlapping subsets of an incomplete data set that contain only populated values. During prediction, each test instance is mapped to one of these models based on its feature-missing pattern. BRM can be applied to any supervised learning model for tabular data. We demonstrate the application of BRM using simulations of blockwise missing patterns on three complete data sets from public repositories.


